# 鬲融：从姚班到普林斯顿

当我们在谈论天才时，我们究竟在谈论什么？

是他们先天就有的聪慧、原生家庭的影响，还是在求学受教中不断得以循循引导的环境？

今天要介绍的人，正是基于这样的大背景。

他自小聪颖，18岁保送清华计算机系，其后恰逢“姚班”创办，受教于图灵奖唯一华人得主姚期智教授，再后来求学于普林斯顿大学，2015年起成为杜克大学80后助理教授。

但就在最近，他有了更新更重要的荣誉：斯隆奖得主。

年仅32岁。

他就是鬲(gé)融，一个出自清华计算机系、代表姚班实力的传说。


△清华时期的鬲融

新晋斯隆奖得主
斯隆奖，1955年设立，由美国艾尔弗·斯隆基金会（The Alfred P. Sloan Foundation）每年颁发一次，旨在向物理学、化学和数学领域的“早期职业科学家和学者提供支持和认可”。

后来陆续增加了神经科学、经济学、计算机科学、以及计算和进化分子生物学。

但斯隆奖之所以全球瞩目，还因为历来有“诺奖风向标”的美誉。

比如“纳什均衡”的提出者约翰·纳什、知名物理学家理查德·费曼，都曾先有斯隆奖殊荣，后来加冕诺贝尔。

迄今为止，共计有47位该奖项获奖人获得了“诺贝尔奖”。另有17位获奖人获得了“数学菲尔兹奖”，69位获奖人获得“美国国家科学奖章”，18位获得“约翰·贝茨·克拉克奖”。




而现在，生于中国河北唐山的年轻计算机科学家鬲融，跻身新晋斯隆奖得主。

由于他基于对深度学习中非凸优化（non-convex optimization）的研究，鬲融获此殊荣。

这项研究实际也与当前人工智能领域最受关注的问题息息相关。

在清华计算机系采访中，鬲融介绍说：

现在机器学习大多使用深度学习算法，这些算法需要通过解决一些非凸优化问题来找到最优的神经网络参数。理论上非凸优化在最坏情况下是非常困难的，但是实际上即使是非常简单的算法（比如梯度下降gradient descent）都表现很好。我最近的工作对于一些简单的非凸优化问题给出了一些分析，可以证明所有的局部最优解都是全局最优解。
更通俗来讲，鬲融的研究，也是揭秘神经网络的一部分，在“大力出奇迹”的神经网络带动AI狂飙突进后，业界都在尝试能打造更精准的神经网络。


△杜克大学刊发鬲融获奖消息

鬲融的研究，走在人工智能全球Top竞技最前沿，并且已经受到权威奖项认可，未来绝对可期。

但对于这样一位年轻科学家，其实很少出现在大众舆论报道中。

相比起鬲融，我们讲了更多其他首届“清华姚班”毕业生的江湖往事，比如教主楼天城。

然而鬲融之于教主，同样星辉灿烂。

甚至楼教主的高中恩师还有过感叹：知道清华鬲融后，才知道天城其实是地道的勤奋型——而不是天才型。

当然，这样的评价有楼教主恩师的谦虚，但也能侧面说明鬲融之才。

作为清华曾经的17科满分大神、特等奖学金得主，鬲融配得起这样的赞誉。

所以，鬲融是谁？




唐山天才
鬲融生于河北唐山，很小就展现出少年天赋。

他父母都是理工类大学生，其后从事技术工作，而且家庭文化气息浓厚，读书是他们家庭生活的一部分，鬲融父母也懂得如何从兴趣引导儿子成长。

学龄前，鬲融就满满对自然世界的好奇，追问父母：天上的星星为什么发光、水中的鱼儿为什么会游泳……

当时见孩子对这些事情感兴趣，鬲融父母就带孩子去北京天文馆、自然博物馆、科技馆等地方，让他尽情地看，一次没看够再去第二次。

后来鬲融父亲鬲红民和母亲刘俊英，把他们的这些做法总结为让孩子“长见识”。“长见识”活动培养了鬲融好学肯动脑的习惯。

更关键的是，鬲融父母知道如何引导支持鬲融的兴趣。


△鬲融高中母校

《新民晚报》在2004年有过这样的揭秘：

鬲融最早的计算机兴趣，源自母亲刘俊英单位。那时小学生鬲融，课余时间经常去母亲工作单位玩。

当时电脑尚不普及，单位只有一台苹果机，但鬲融只要一到单位就缠着妈妈带他去有电脑的办公室。

他对电脑着了迷，在电脑前一坐下来就像生了根。妈妈单位懂电脑的那位同事也很喜欢这个很有灵气的孩子，对鬲融有问必答，刘俊英发现鬲融对电脑知识接受得特别快，做母亲的一方面感到高兴，另一方面又在思考怎样把这个兴趣培养成孩子的特长。

从上小学6年级开始，鬲融正式开始跟老师学习电脑。




那时候父母便给他买了一部学习机，半年多后，随着鬲融所学知识的深化，学习机已不够用了，他们又花6000多元为他买来一台电脑。

一开始，鬲融的父亲电脑水平要比鬲融高，于是父亲又做了儿子的老师，爷俩互相切磋，长进很快。

但鬲融很快展现出少年天才的勤奋专注。

有一次，鬲红民买了一本有关“C语言”的书籍准备学习，但感觉挺繁琐的，便置之一旁，可没多久，发现儿子正在认真地读那本书，一问，儿子把上面的内容差不多全搞明白了。

每次放寒暑假前，鬲融的父母都关心地问鬲融假期想研究些什么，随后带孩子到书店里转一转，选上一本或几本鬲融感兴趣的计算机的书籍，很多计算机知识都是鬲融利用假期自学而成的。

在此次斩获斯隆奖后，鬲融也在清华计算机系采访中谈到父母的作用。

他说，上世纪90年代，当同龄人尚不知晓计算机为何物的时候，他就拿到了打开神奇世界的钥匙，家里不仅很早就给他买了电脑，而且父亲还亲自教他编程，让他对计算机的兴趣始终保持至今。

清华传说
跟诸多清华姚班学生一样，鬲融与清华结缘，始于计算机竞赛。

从高中开始，鬲融就参加全国性的计算机竞赛，2002-2004年期间，鬲融连续两届获得全国信息学奥林匹克竞赛金牌。

并且由此入选2004年国家代表队，然后在当年希腊雅典举行的第16届国际信息学奥林匹克竞赛中，为中国斩获国际金牌。


△鬲融（左一）

那一年，鬲融的国家队队友还有杭州十四中的楼天城，他们其后一起被保送清华大学计算机系，并进一步成为首届清华姚班同学。

鬲融在最近清华计算机系采访中还玩笑说：之所以花落计算机系，是因为保送没得选。

但他也补充——如果有得选，必然也是计算机系。

而且清华入学后，鬲融对计算机的兴趣也有增不减，而且因为环境氛围，更是与日俱增。


△鬲融代表清华姚班出国交流分享

由他和他的同学组成的三人团队，从中国打出去取得ACM亚洲赛区第一名后，最终获得国际ACM竞赛第二名。

当然，后来提到ACM，我们更多记住了教主楼天城。

但当时在清华和姚班，鬲融光辉同样夺目。

关于鬲融，至今有这样三个传说：

17科满分
学分积三年排名第一
计算机系历史最高GPA
而且连楼教主的高中恩师，在探访后也对鬲融之天才，由衷敬佩。

后来在采访中，楼教主的高中信息学老师李建江回忆说，那时只要去北京出差有时间，就会顺路到清华看望得意门生。

由此对清华的天才云集大为震惊。




李老师称，一直以来，都觉得楼教主这样的学生应该是天才型，但到了清华后听其老师同学之言，才知道鬲融这样的存在。

作为楼教主同班同学，鬲融学习成绩始终全年级第一，而教主只能屈居第二到第六。

教主恩师还举例，那时晚上21点去清华计算机系宿舍，鬲融肯定在，但天城还在教室用功。

他这才感叹，相比鬲融，教主是地道的勤奋型。

不过教主恩师自然也是谦虚至极，才有这样的感悟，而且天才与勤奋，原本就是紧密相关，“宿舍在与不在”，多半是为了强调教主天才也勤奋。

但也能从侧面窥见鬲融之才华。

而且现在两开花。楼教主由学入产业界，成为全球自动驾驶最知名的创业代表，而鬲融则在科研领域，不断推进AI最前沿研究。

清华姚班，名不虚传。




姚班点金
鬲融亦毫不掩饰对于清华姚班经历的感激。

他告诉清华计算机系采访者：清华是他真正的起点，他在那里打好了基础，而且找到了自我，也提升了自信。

鬲融举例，当时有很多数学课程其实也不太知道有什么用，但是后来在研究中会回想起来，才后知后觉意识到原来它的作用是润物无声的那种，化为了思维中的一部分，这就是清华严格的基础课程的力量。

还有在姚班的时候，老师有时会布置一些任务，最开始会觉得这可能不行吧？这能做到吗？然而当自己真正努力去做了，它反而并没有想象中那么难，有很多不擅长的东西（像跑3000米什么的）也不是完全不能做到。就这样慢慢去突破，去尝试，自我的极限就找到了，而自己的自信心也有了很大提高。

毫无疑问，清华和姚班，让鬲融见到高山。




他也展露虚怀一面，说当时计算机系姚班有太多有才华的同学了，对比之下自己只能算是打酱油的。

“大家在一起的氛围非常好，现在同学之间一半都会在学术会议上见到，或者出差去加州或者其他城市的时候见一下。我们聚在一起还经常聊最近的情况，聊在计算机系读书时同学们之间的趣事。”

鬲融还怀念起清华传统活动智能体竞赛：

我以前在系科协工作过，当时每年都有智能体竞赛，我们组织的那次记得最后决赛是楼天城（楼教主）和栗师两个人的AI对决，非常精彩。
此外，鬲融也分享了科研突破的“秘诀”，特别是在当前大热的AI领域，沉心静气之重要：

最近机器学习这边热度很高，感觉整个领域都有一点浮躁，论文也越来越多。
今后，我希望能够静下心来做研究，做出更有意义的结果。这算是我对自己的一种提醒吧。
再评姚班
最后，是否也能在此时再看清华姚班意义？

2004年，姚期智教授受杨振宁院士力邀回国，辞去普林斯顿大学终身教职来到清华大学任教。

此前几年，姚期智院士就在国内跟周立柱教授、张钹院士等清华老师有过交流，对国内求师若渴印象深刻。

后来清华领导也正式邀请姚期智教授回国任教，到清华做计算机系方面的研究，弥补中国在这方面与世界领先水平的差距，并让姚期智教授的少年偶像杨振宁院士亲当“说客”。


△杨振宁院士和姚期智院士

最终，姚期智教授为诚感动，也希望为国培养计算机人才，终于加盟清华。

但后来回忆起来，姚期智教授认为提出创办“姚班”，是一个大胆的尝试。

回国不久，姚期智教授就提出希望成立一个本科班，能够让它变成世界上最出色的本科班之一，且认为清华有中国最出色的中学生，人才条件上不成问题，而且中国学生各科都很好，不存在课程定义方面的大挑战，最后还能展开诸多跨学科跨领域科研。

当时清华大学领导听后就同意了，还告诉姚教授集中精力在教学团队上就好，其他的事情他们去推进。

后来，清华姚班在2005年正式推出，首届招募24名本科生。


△首届清华姚班录取名单

再后来的事情，外界就知道了，清华姚班渐渐打响名气，成为中国计算机人才的最佳培养模式之一。

然而回想起来，过程中不是没有阻力。

当时在清华和学界，始终伴随“搞精英教育”的讨论。

一直以来，教育的核心议题之一是让更多人公平接受相同资源。姚教授开风气之先在清华这样一所中国最高学府“单辟一班”，不可能没有反对声音。

而且教学讲究循序渐进，并不是一年两年就能看到成效，更何况中国本科教育，更多处于通识、引路和找方向阶段，要追求立竿见影更是难上加难。

幸好姚期智教授很坚定，清华大学领导也很坚定




姚教授后来还说：

一切顺其自然也可以，中国经过30、50年发展也能实现，但我们不能等待了，我们这一代人都等不起了，国际环境也不允许我们顺其自然发展，我们必须要用很短的时间做好科技方面的良性循环。
现在，清华姚班开始展现成果和良性循环。

在学界，有鬲融这样的青年领军人才，做着全世界最前沿的研究。

在产业界，楼教主、印奇、唐文斌，杨沐等创办的AI公司，不仅全球瞩目，而且还吸引更多后辈精英，加速人才培养和循环。

姚期智教授也通过出任学生公司的首席顾问，把产学研打通，让在校学生通过参与产业最前沿落地打磨技术增长见识，也利用科研优势帮产业技术更上一层楼。


△姚期智教授出任旷视首席顾问图

姚期智教授的夙愿正在无限扩大影响力，清华姚班的影响力也源源不断扩大于清华校园之外。

现在，距离清华姚班创办14年、首届学生毕业11年之际，再也不用争论“清华姚班”模式是否相悖于教育公平了。

而且本质上说，苛于一碗水端平的教育，实际也是对人才天赋的浪费。

幸好2004年杨振宁院士成功劝说姚期智院士加盟，幸好2005年姚班顺利推出，也幸好鬲融楼天城等赶上这趟当年前途未卜的车。

虽然也是一段实验之旅，但现在成果初现。

未来，中国的诺贝尔奖得主、图灵奖得主，也有可能诞生在姚班学子中间。

嗯，值得期待~

参考报道：

清华计算机系专访：鬲融 | 14年前“清华姚班”的天才少年，如今的“斯隆奖”获得者

https://mp.weixin.qq.com/s/qu8hm43X4L8Pi7NTU0Whww

新民晚报报道：小电脑迷成为信息学高手——探访奥赛金牌获得者鬲融的成功“秘诀”

http://news.sina.com.cn/c/2004-09-20/15133719309s.shtml

杜克大学报道：Professor Rong Ge Awarded Sloan Fellowship

https://www.cs.duke.edu/news/articles/3282







作者：雷峰网
链接：https://zhuanlan.zhihu.com/p/385463445
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



列举从信息学竞赛（OI）或清华计算机系走出来的牛人，人们总会提到鬲融的名字。



这位来自河北唐山的青年，因2004年与楼天城、[胡伟栋](https://www.zhihu.com/search?q=胡伟栋&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"385463445"})、栗师代表中国参加第 16 届国际信息学竞赛（IOI）、全面夺金而一举成名，保送清华后又在卧虎藏龙的计算机系留下三项至今无人打破的纪录：17科满分、学分积三年排名第一、计算机系历史最高GPA。



当你以为他只是一位竞赛强人时，他向你展示了在文化综合科上的实力；当你以为他只是“两耳不闻窗外事，一心只读圣贤书”的学霸时，他又在离开清华多年后捧回在理论研究上的拔群战绩：NIPS 2016 最佳学生论文奖、素有“诺贝尔风向标”之称的斯隆研究奖…



然而，关于鬲融的传说，大多还是集中在早期的竞赛与清华姚班的学习上。相比之下，他去普林斯顿读博、从事理论研究的经历则鲜为人知。



作为“光环学生”，[鬲融](https://www.zhihu.com/search?q=鬲融&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"385463445"})的一言一行被寄予厚望。但是，在与鬲融的对话中，我们发现，这位昔日的 IOI 战神、清华本科特等奖获得者在科研上并非一帆风顺。刚入门时，他也不知道该如何做科研，也是经过一番自我觉醒，才明白了其中的门路。



与竞赛、考试相比，鬲融在科研上属于“大器晚成”：读博前三年，他在近似算法研究上探索无果，无奈转向机器学习理论研究，最后两年才发了顶会文章。到2019年凭借[非凸优化](https://www.zhihu.com/search?q=非凸优化&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"385463445"})的研究贡献获得斯隆研究奖时，他已是杜克大学计算机系的一名“青椒”。



2008年，鬲融从清华大学本科毕业，随后赴普林斯顿大学读博、[微软研究院](https://www.zhihu.com/search?q=微软研究院&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"385463445"})新英格兰分部担任博士后，2015年进入杜克大学担任教职。从姚班开始立志做理论研究，到成为机器学习理论研究方向小有名气的青年学者，鬲融用了近 10 年。



那么，鬲融离开清华后的成长历程是怎样的？今天，我们只谈鬲融与理论研究之间的故事。

作者 | 陈彩娴



### **1.** **普林斯顿前半章**

在清华计算机系 4 字班（2004级）中，最出名的当数信息学竞赛圈无人不知的楼天城“楼教主”，百度曾经最年轻的 T10 级员工，后来又率先创立了国内知名的自动驾驶公司小马智行（Pony.ai）。许多人最初知道鬲融，是借楼教主的名声，因为在楼教主的一段轶事里，鬲融曾作为一个“配角”的身份出现：当时，楼教主的高中信息学竞赛教练李建江一直认为楼教主是天才型学生，心中引以为豪，每次去北京出差，只要有时间就会顺路去清华看望这位得意门生。结果到了清华，与老师、同学交流，李教练发现，自己的学生在计算机系最多只能排到第二名，因为楼教主的同班同学鬲融常年排名全年级第一。他还举例：每次夜晚 9 点去清华的计算机系宿舍，鬲融肯定在，而[楼教主](https://www.zhihu.com/search?q=楼教主&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"385463445"})还在教室用功。他因此感叹，相比鬲融，楼教主是地道的勤奋型选手。在与**AI科技评论**的对话中，鬲融首次回应了这段传闻：“哈哈其实是因为当时我们宿舍有空调，所以就不用去教室学习，楼天城他们宿舍没有空调，他只能去教室学习。”



![img](https://pic4.zhimg.com/v2-798fcd960c8c85f4e4fd397a507a0a83_b.jpg)



图注：2007年，鬲融（中间）与楼天城（最左）、胡伟栋（最右）在日本参加ACM/ICPC，获得亚洲赛区冠军、全球第二名

楼天城的天赋与能力毋庸置疑，但相形之下，鬲融的实力也可见一斑。然而，在理论研究领域深耕多年后，回头再看在清华读本科时的成绩与排名，鬲融只是一笑置之，称自己不过是有一点“考试的天赋”：

> 我就是在做一些不是特别难的题时可以做得很快，也不太会出错。考试可能比较有用，但是（这项能力）后来到了研究上面就没有什么用了。研究的题比考试难，有些人可能考试时会在一些简单的题目上卡住，但在做研究的难题时就会做得很快。

鬲融与楼教主曾经是2004年一起参加 IOI 的战友，上了清华后又曾两次组队参加编程竞赛（两岸清华编程比赛与ACM/ICPC）。但是，与业余时间还爱“玩玩竞赛题”的楼教主相比，鬲融并不“恋战”，参加完2007年ACM/ICPC后便彻底告别了竞赛圈，因为那一年，他找到了下一个人生目标：理论计算机研究。当时，鬲融刚加入姚班不久。在姚期智、陈卫、孙晓明等人的引导下，尤其是姚期智亲自讲授《[理论计算机](https://www.zhihu.com/search?q=理论计算机&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"385463445"})》课程，鬲融迷上了理论研究，立志走学术研究道路，将科研作为毕生之所向。但是，与竞赛、做题相比，鬲融的科研“天赋”似乎略微逊色。比如，读博前期，鬲融在近似算法（Approximation Algorithm）的研究课题上苦苦折腾了三年，也没有找到正确的方向，最后只能无奈放弃。2008年，在姚先生的建议下，鬲融去了普林斯顿大学（计算机理论研究排名全美前5）读博。普林斯顿的计算机系每年只招收大约 20 名学生。在鬲融那一届，除了他，还有 3 名中国学生被录取，包括鬲融昔日的 IOI 战友栗师（现任纽约州立大学布法罗分校计算机系副教授）。

![img](https://pic2.zhimg.com/v2-8e7a4e18071af4a08a1f77885613d595_b.jpg)

图注：普林斯顿大学清华姚班出来的学生对研究往往有一种使命感， 比如，引领一个领域的新潮流，或解决一道历史上悬难已久的问题。年少的鬲融起初对学术研究也是这样一种想法：“世界上有那么多猜想与没解决的问题，挑一个去做就是了。”近似算法的研究历史可以追溯到18世纪中期欧拉（L.Euler）研究的骑士环游问题，目标是用近似方法在多项式时间内给出尽可能接近最优值的解，比如著名的「旅行商问题」（TSP）：一个商品推销员要去若干个城市推销商品，该推销员从一个城市出发，需要经过所有城市后，回到出发地，那么，TA 应如何选择行进路线，以使总的行程最短？这个课题很吸引鬲融。但很快，他就感到“出师不利”。近似算法发展至今，亟待解决的问题是大家都知道的几个问题，比如旅行商问题、染色问题、最小分割等。鬲融的工作就是研究如何解决这些问题。但是，虽然有明确的研究方向，他却总会在各种地方卡住，导致工作无法进行下去。至于卡住的原因，鬲融坦言，他到现在也还不是很清楚：

> 可能是对研究的课题不熟悉，也可能是思路不对，各种可能都有。我们当时想做的事情直到现在也还没有人做出来，所以也有可能是因为选择的题太难。

三年下来，虽然他在ICALP、ISAAC等理论计算机的会议与期刊上发表了论文，但总体感觉还是困难比较多，所取得的成果也远远没有达到鬲融对自己的要求。回想当时的磕磕绊绊，鬲融分析，做研究无非就是两方面：一是找到合适的题目，二是把这个题目做出来。在选择近似算法时，他对第一步的认知只是在“世界上已有的难题”上，直到后来转向机器学习理论研究，才发现：原来学会自己定义问题，也是一项可贵的研究能力。

### **2. 科研转折点与引路人**

转折点发生在 2012 年。那一年，Hinton与他的学生Alex在ImageNet比赛中凭借AlexNet远超第二名10个百分点，勇夺冠军，深度学习崛起。鬲融的博士导师 Sanjeev Arora敏锐地察觉到机器学习（尤其是深度学习）在未来的发展潜力，开始关注机器学习。当时，鬲融正在近似算法的课题上挣扎，这正好给了他重新选择的契机。刚好他本科在微软亚研实习时也接触过机器学习，对这个方向也很感兴趣，于是就选择了转向研究机器学习理论。在这里不得不提的一点是，Sanjeev Arora 在鬲融读博期间对他产生了重要影响，不仅直接引导他走进了机器学习研究领域，也塑造了他做科研的方法与态度。Sanjeev Arora是普林斯顿大学计算机系的Charles C. Fitzmorris教授，以研究概率可检验证明（尤其是PCP定理）而闻名，1996年获得斯隆研究奖，2001年与2010年共两次获得哥德尔奖（理论计算机领域最高奖），2012年又获得西蒙斯研究奖与福尔克森奖（离散数学领域最高奖），是理论计算机研究领域有名的翘楚。

![img](https://pic2.zhimg.com/v2-b74c20c7d0549d14a7e173e127957c49_b.jpg)

图注：Sanjeev Arora鬲融是 Arora 门下的第一个中国留学生。在鬲融来到普林斯顿的前一年（2007年），Arora 与 Satyen Kale（现任谷歌研究科学家）刚刚用乘法权更新算法（Multiplicative Weight Update Method）的矩阵版本求解了 SDP，并对一些问题给出了更快的近似算法。MWU 的特点是理论复杂，但算法简洁。Arora 在近似算法上“大道至简”的追求，吸引了鬲融。截至目前，Arora 只带过 3 名中国学生，除了鬲融，其余 2 位是马腾宇与李远志，后来都成为了机器学习领域的佼佼者。马腾宇与李远志也是清华大学的校友，分别在2012年、2013年来到普林斯顿读博，是鬲融日后的重要研究合作者。马腾宇毕业后到斯坦福大学任教，2021年也凭借在非凸优化上的研究成果获得了斯隆研究奖，而李远志毕业后加入了卡内基梅隆大学机器学习系担任助理教授。在鬲融的眼里，Sanjeev Arora是一位各方面都让人佩服的学者：

> 在转向机器学习之前，他在近似算法及其复杂度的研究上已获得非常出色的成就。很多人可能在某个方向上做出成果，就会沿着这个方向继续做一辈子，但他是一个很喜欢研究新东西的人，喜欢挑战自己，每隔几年就会换一个新的方向，然后每个方向都能取得不错的成就。当时转向机器学习时，他在第一年或第二年就做出了很好的结果。

也是因为 Arora 的这项品质，他在2012年转向机器学习研究时，促使鬲融等人也注意到了机器学习，直接改变了鬲融的研究方向。2012年转向机器学习时，鬲融已是一名博“四”生，开始一个全新的方向需要极大的勇气。但他二话不说，重新调整了自己的方向。出乎意料的是，转变方向后，他的研究进展异常顺利，最后两年连续发表了 8 篇顶会论文，其中理论计算机顶会 FOCS 就有 2 篇、STOC 有 1 篇，远远超过了博士前三年的成果总和。与近似算法不同，机器学习是一个相对较新的领域，有许多新的问题。从鬲融的角度来看，这时他的研究问题变成了：如何把一个实际的机器学习问题放到理论的框架里讨论？在这个过程中，“自己定义问题”的重要性明显上升。拿鬲融转向机器学习研究后的第一个工作举例。当时，鬲融在微软研究院新英格兰分部实习，参与主题建模（Topic Modeling）的研究工作。主题建模被用于对数据（网页、新闻、图片等等）进行自动理解与分类，在理论研究上侧重于学习模型的参数。当时的方法大多依赖于奇异值分解（SVD），但SVD方法有两个限制：要么假设每篇文章只包含一个主题，要么只能恢复主题向量的范围，而非主题向量本身。针对 SVD 用于主题建模的局限性，鬲融与 Arora 等人提出了一个问题：“如果没有真正的矩阵 AW ，而是从每一列所代表的分布中得到一些样本（比如 100 个样本），怎么办？”他们假设并证明了 NMF（[非负矩阵分解](https://www.zhihu.com/search?q=非负矩阵分解&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"385463445"})）比 SVD 更适用于主题建模，并利用 NMF 获得了第一个没有上述两个限制的[多项式时间算法](https://www.zhihu.com/search?q=多项式时间算法&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"385463445"})，该算法可以泛化至包含主题与主题相关的模型，比如相关主题模型（Correlated Topic Model）与弹珠机分配模型（Pachinko Allocation Model）。最后，他们的工作（“Learning Topic Models - Going beyond SVD” ）发表在 FOCS 2012 上。这也是鬲融在 FOCS （[理论计算机方向](https://www.zhihu.com/search?q=理论计算机方向&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"385463445"})中稿难度最高的会议之一）上发表的第一篇论文。

![img](https://pic3.zhimg.com/v2-a839edd646f250a5d1459052132830f2_b.jpg)

地址：[https://arxiv.org/abs/1204.1956](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1204.1956)之后，他又在主题建模的研究上陆续发表了几篇文章，包括被 ICML 2013 录取的工作“A Practical Algorithm for Topic Modeling with Provable Guarantees ”，在业内引起不小关注，积累了一点名声。在理论研究领域摸爬滚打多年后，鬲融发现：重要的问题并不一定是很多年前就有人提出来的，提出问题本身也是一个重要的研究方向；在做研究时，如果一个问题进展不顺，不一定是你的研究技术不对，也有可能是你提的问题本身就是错的。这也是鬲融在读博期间的主要收获：对研究形成了一个比较完整的认知，并学会了如何选择一个适合自己的题目。鬲融能够“守得云开见月明”的另一个重要因素是坚持。而这一品质，也主要是受到 Arora 的影响。鬲融回忆，在读PhD时，他在研究问题上卡住时，虽然会花时间去想，但经常会有一种感觉，就是“这个想法好像不行，做不下去”，便想放弃。在每周的组会上，他与 Arora 讨论卡住的点，说不知道该怎么做时，Arora 都会说：“这只是一点困难，你可以换一个思路，尝试别的解决方法。”“如果要放弃正在进行的方向，就要给出严谨的证明，让 Arora 相信这个方向确实做不了。但是，只要没有证明这个方向不行，他就不会放弃，会不停地想各种解决办法。”鬲融形容，“在这种精神下，后来我也确实解决了一些卡住的问题。”大约是受到 Arora 的鼓舞，鬲融渐渐懂得了坚持，面对难题时也会乐观许多，更倾向于觉得“这个课题是可以做的”而不是“这个想法好像不行”，即使题目暂时没有做出来，也不会轻易放弃，而是坚持到实在做不下去的时候。他感叹：“如果当时我一说某个思路有哪些困难、觉得做不下去，Arora 就说我们不做这个题了，那么现在的结果肯定会不一样。” 

### **3. 大器晚成的 IOI 战神**

但是，尽管最后两年发表了一些论文，与竞赛、本科时的辉煌成绩相比，鬲融的博士生涯还是相对黯淡：没有大厂奖学金，没有最佳论文。换作旁人，博士期间能在理论计算机顶会 FOCS 与 STOC 上发表3篇工作，已经非常了不起，但对这位清华特奖获得者来说，总觉得还缺点什么。鬲融在2013年获得博士学位。当时，他刚刚在机器学习理论的酒席上喝到微醺，意犹未尽，“感觉还有很多事情想做”，于是就决定去之前实习的微软研究院新英格兰分部做博士后。也是在两年的博士后期间，鬲融开始了在非凸优化（Non-Convex Optimization）方向的研究，为之后获得斯隆研究奖打下了基础。在他还是一名实习生时，微软内部就有人在研究用张量分解（Tensor decompositions）做话题建模。他们的技术非常神奇，就是用两个矩阵乘一下，然后做一下对角化就能得出成果，光看论文本身完全不明白为什么这么做会有用。鬲融就很好奇：“为什么张量分解这么厉害？我不知道有什么理由，所以我就想去研究。”于是，他们尝试用张量分解来研究话题模型上的参数问题，发现张量分解不仅可以用于解释话题模型的参数问题，还可以解释与话题模型类似的机器学习模型的参数问题。他们的工作“Tensor decompositions for learning latent variable models”最后发表在了机器学习顶刊 JMLR上。



![img](https://pic1.zhimg.com/v2-b5871ea373c3c8e5b33799c08cffbab4_b.jpg)



地址：[https://arxiv.org/abs/1210.7559](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1210.7559)

他们在这方面做了很多工作，也取得了不错的成果，但用鬲融的话说，就是“做多了，也就没那么有意思了”。所以，到了博士后阶段，他就开始寻找新的方向。他从张量分解出发，无意间发现了一个新的研究课题，就是非凸优化（non-convex optimization）。当时，他发现在张量分解的算法中，比如张量有10个部分，当时的算法是一个部分、一个部分地找，但有时候，我们会想同时找出这10个部分，这时就需要用到优化技术。那时大家常用的随机梯度下降优化方法并不管用，于是他就花了很长时间研究如何转换一个目标函数，可以使它的效果更好。鬲融回忆：“可能是运气比较好，在寻找、测试目标函数时，我首先找到了一个目标函数，使得这个优化方法可以把所有的张量部分同时找出来。分析随机梯度下降的时候，我们研究出了一套新的分析方法，后来发现这套分析方法非常有用，不止对我们研究的张量分解问题有用，对许多其他问题也有用。”接着，他与袁洋、金驰、黄芙蓉等人沿着这个方向继续研究非凸优化的函数。在许多情况下，非凸函数的目标是找到一个合理的局部最小值，主要的问题是梯度更新被困在鞍点（saddle points）中。他们尝试辨析非凸优化问题的鞍点性质（如果函数没有退化的鞍点，那么对梯度做轻微的扰动就可以逃出鞍点)，以进行有效优化。利用这个属性，他们发现随机梯度下降可以在多项式迭代中收敛到局部最小值。这是第一项为在具有多个局部最小值和鞍点的非凸函数中的随机梯度下降提供全局收敛保证的工作。他们的工作开拓了一个新的研究方向，其成果“Escaping From Saddle Points --- Online Stochastic Gradient for Tensor Decomposition”被机器学习理论会议 COLT 2015 收录，吸引了许多人往这个方向努力，并获得了许多新的结果。



![img](https://pic4.zhimg.com/v2-d8cfbe7098b89783f788877eb347dd1b_b.jpg)



地址：[https://arxiv.org/abs/1503.02101](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1503.02101)

这是鬲融没有想到的：“我感觉还是挺幸运的，我从一个非常特殊的问题出发，但是我们最后得到的结论是非常广泛的，研究也受到不少重视。”这项工作对机器学习理论研究领域的贡献主要有两个：一是证明了张量分析中他新提出来的目标函数有一些好的性质，比如它没有坏的局部最优解，它的鞍点也有一些性质；二是证明了我们可以用很简单的算法（如梯度下降）来优化所有具备这种性质的目标函数。也是凭借这两个主要贡献，鬲融在2019年获得[斯隆研究奖](https://www.zhihu.com/search?q=斯隆研究奖&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"385463445"})。后来，他又分别在这两个贡献上作了进一步的研究。比如，在第一个贡献上，他们后来证明更多函数都具备类似性质，包括与马腾宇、Jason Lee等人合作的那篇工作“Matrix Completion has No Supurious Local Minimum”（获 NIPS 2016 最佳学生论文）也证明矩阵补全（matrix completion）没有坏的局部最优解。据说，鬲融与马腾宇合作的这篇工作从开始构思到完成投稿，前后只用了不到两个月时间。那时 COLT 2015 的工作刚发表不久，可以借鉴一二。鬲融回忆：“当时做的时候，我们就很有信心，因为我们三个人都觉得这个东西肯定是对的。马腾宇也很快就有了一些具体的想法，我们按照一些步骤去做，然后挺顺利地就做出来了。”至此，鬲融已成为研究用非凸优化寻找最优神经网络参数的早期开拓者之一。但是，在2019年获得斯隆研究奖后，鬲融又像2004年拿到IOI金牌一样，若无其事地回到了原本的生活轨迹上，做一名安安静静做研究的教师。

![img](https://pic2.zhimg.com/v2-05ec63492c05810f4d6131d3d7ab14d5_b.jpg)

图注：鬲融目前在杜克大学任教

斯隆研究奖每年表彰一次，在以往的获奖人员中，有47人后来获得诺贝尔奖、17人获得[菲尔兹数学奖](https://www.zhihu.com/search?q=菲尔兹数学奖&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"385463445"})、69位获得国家科学奖、18位获得约翰贝茨克拉克经济学奖。史上许多著名的科学家都曾获得斯隆研究奖，包括物理学家理查德·费曼 ，默里·盖尔曼，以及博弈论学家约翰·纳什。从2008年清华毕业，到获得斯隆研究奖，鬲融用了 10 年。在这期间，他在 4 字班的许多同学（如楼天城、贝小辉）都已早早在新的领域声名鹊起，但人们谈起鬲融，仍只是围绕竞赛与GPA。虽然鬲融在中途沉寂了很长时间，但在姚班创始人、中国首位图灵奖得主姚期智姚先生的心中，他的名字一直是姚班教育的骄傲。在2017年鬲融还没有获得斯隆研究奖时，姚先生谈起姚班教育，首先就提到了他的名字：

> 在学界的，我们有好几个做人工智能的学生，已经在大学任教的有两个，一个是在美国的杜克大学，一个是在美国的斯坦福大学做教授，他们都从事人工智能理论基础方面的工作。他们在过去的四五年，在人工智能理论方面已经非常非常出色……他们确实可以说在人工智能领域是先驱，将来一定会在该领域留下非常深刻的痕迹。

其中，在杜克大学任教的便是鬲融，而在斯坦福任教的则是鬲融的同门师弟马腾宇。听闻姚先生的挂念，当时离开清华多年的鬲融心中感触万分：“我感觉挺感动的，因为姚班出来很多很强的人，远远不止我们两个。”

![img](https://pic2.zhimg.com/v2-9a0d1845ad2faf214be98093e74ca2bd_b.jpg)

图注：2019年，鬲融回清华交叉信息研究院（即“姚班”）作学术报告

在鬲融的成长路上，姚班的身影其实从未远离。他提到，之前在姚班所学习的知识、思路，一开始不知道有什么用，但后来都用上了，甚至后悔“当初怎么不多学点”。而曾经的同窗好友虽然选择了不同的人生方向，“但想到大家跟我一样都在努力，就觉得蛮开心的。”

### **4. 理论研究的意义**

“对我个人来说，如果我知道一个算法，但是我不知道它的工作原理，是一件不太高兴的事情，所以我自己主要就是因为好奇才选择做机器学习理论研究。”问及从事理论研究的意义，鬲融这样谈道。而从整个机器学习领域的发展来看，理论机器学习的研究主要有两个意义：

> 一是如果知道[神经网络算法](https://www.zhihu.com/search?q=神经网络算法&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"385463445"})的工作原理，我们就有希望解决一些问题，比如让它变得更快，或者用更少的资源；二是可以解决人们关心的一些实际问题，比如计算机视觉中神经网络的弱关性问题，把一张图片错误识别为其他图片。

在深度学习时代，机器学习算法尝试从文本、图像等数据中自动学习有用的隐含表示。近年来，鬲融的研究重点是希望通过非凸优化与张量分解研究如何设计高效的算法找到这些隐含表示，比如神经网络模型中的超参数化。目前的一个观点是：有了超参数化后，优化会变得简单。有些工作也得到了同样的结果，但还有很多问题是未知的，比如：神经网络要多大，才能有足够好的优化性质？有些观点认为神经网络要无穷宽，鬲融团队的研究课题则是：你的神经网络不需要无穷宽，只要足够宽就可以证明一些类似的性质。他们最近做了一个工作（“Guarantees for Tuning the Step Size using a Learning-to-Learn Approach”），从理论角度研究如何通过机器学习方法来设计新的优化算法，得出了一个有意思的结论：对于优化问题，如果你用最基本的back-propagation（反向传播）方法来算，它的梯度可能会算不准，如果用其他的方式算，可能还可以算得更精确一些。在未来，他希望能够进一步了解神经网络的优化性质，然后，在掌握足够多的性质后，可以设计出更好的算法。对于想要从事理论研究的学生，鬲融的建议是最好先加入一个研究组去做具体的项目，一是看自己适不适合，二是对机器学习领域的发展有更具体的了解，日后做研究时能更好地定义研究问题。作为最早进入机器学习领域的研究者之一，鬲融能明显感觉到近几年来该领域的飞快发展，论文投稿数量呈指数级增长，给人一种浮躁的感觉。由于很难找到足够多的、有经验的审稿人来支持大规模的会议投稿，导致会议论文的结果有些随机。面对这一现象，鬲融感叹他也难有作为，只能对自己和自己的学生有一个基本要求，就是投出去的论文至少要达到自己满意的标准。随即，鬲融又说：“虽然我对文章的要求严格，但在担任审稿人时，我感觉自己给分还是偏高的。”所谓「取其上者得其中，取其中者得其下」，鬲融在非凸优化与张量分解上的研究成就看似偶然，追溯根源，其实在于他对自己做研究的高要求：对好奇的问题刨根问底，对完成的工作精益求精，耐心、敏锐又谦逊，则成事只在时日长短。科研前期的艰难探索也许是必经之路，即使智如鬲融也不例外。读博三年还没有“像样”的成果？别慌，坚持一下，说不定你也能拿斯隆研究奖。





